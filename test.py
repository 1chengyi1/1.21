import streamlit as st
import pandas as pd
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score
import numpy as np
import random
import plotly.graph_objects as go

# ==========================
# æ•°æ®é¢„å¤„ç†å’Œé£é™©å€¼è®¡ç®—æ¨¡å—
# ==========================
@st.cache_resource(show_spinner=False)
def load_data():
    # è¯»å–åŸå§‹æ•°æ®
    papers_df = pd.read_excel('data2.xlsx', sheet_name='è®ºæ–‡')
    projects_df = pd.read_excel('data2.xlsx', sheet_name='é¡¹ç›®')
    return papers_df, projects_df

def build_networks(papers, projects, weights):
    G_authors = nx.Graph()

    def add_edges(df):
        for _, row in df.iterrows():
            weight = weights.get(row['ä¸ç«¯åŸå› '], 1)
            G_authors.add_edge(row['å§“å'], row['ä¸ç«¯å†…å®¹'], weight=weight)

    add_edges(papers)
    add_edges(projects)

    # å…±åŒé¡¹ç›®/è®ºæ–‡è¿æ¥
    for df in [papers, projects]:
        for _, row in df.iterrows():
            authors = [row['å§“å']]
            weight = weights.get(row['ä¸ç«¯åŸå› '], 1)
            for i in range(len(authors)):
                for j in range(i + 1, len(authors)):
                    if G_authors.has_edge(authors[i], authors[j]):
                        G_authors[authors[i]][authors[j]]['weight'] += weight
                    else:
                        G_authors.add_edge(authors[i], authors[j], weight=weight)

    # ç ”ç©¶æ–¹å‘ç›¸ä¼¼æ€§è¿æ¥
    research_areas = papers.groupby('å§“å')['ç ”ç©¶æ–¹å‘'].apply(lambda x: ' '.join(x)).reset_index()
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(research_areas['ç ”ç©¶æ–¹å‘'])
    similarity_matrix = cosine_similarity(tfidf_matrix)

    for i in range(len(research_areas)):
        for j in range(i + 1, len(research_areas)):
            if similarity_matrix[i, j] > 0.7:
                a1, a2 = research_areas.iloc[i]['å§“å'], research_areas.iloc[j]['å§“å']
                G_authors.add_edge(a1, a2, weight=similarity_matrix[i, j])

    # å…±åŒæœºæ„è¿æ¥
    institution_map = papers.set_index('å§“å')['ç ”ç©¶æœºæ„'].to_dict()
    for a1 in institution_map:
        for a2 in institution_map:
            if a1 != a2 and institution_map[a1] == institution_map[a2]:
                G_authors.add_edge(a1, a2, weight=1)

    return G_authors

def deepwalk(graph, walk_length=30, num_walks=200, embedding_size=128):
    walks = []
    nodes = list(graph.nodes())
    for _ in range(num_walks):
        random.shuffle(nodes)
        for node in nodes:
            walk = [str(node)]
            current = node
            for _ in range(walk_length - 1):
                neighbors = list(graph.neighbors(current))
                if neighbors:
                    current = random.choice(neighbors)
                    walk.append(str(current))
                else:
                    break
            walks.append(walk)

    # ä½¿ç”¨ skip-gram è®­ç»ƒåµŒå…¥
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.decomposition import TruncatedSVD

    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform([' '.join(walk) for walk in walks])

    svd = TruncatedSVD(n_components=embedding_size)
    embeddings = svd.fit_transform(X)

    return {node: embeddings[i] for i, node in enumerate(vectorizer.get_feature_names_out())}

@st.cache_resource(show_spinner=False)
def process_risk_data():
    # ä¸ç«¯åŸå› ä¸¥é‡æ€§æƒé‡
    misconduct_weights = {
        'ä¼ªé€ ã€ç¯¡æ”¹å›¾ç‰‡': 6, 'ç¯¡æ”¹å›¾ç‰‡': 3, 'ç¯¡æ”¹æ•°æ®': 3, 'ç¯¡æ”¹æ•°æ®ã€å›¾ç‰‡': 6,
        'ç¼–é€ ç ”ç©¶è¿‡ç¨‹': 4, 'ç¼–é€ ç ”ç©¶è¿‡ç¨‹ã€ä¸å½“ç½²å': 7, 'ç¯¡æ”¹æ•°æ®ã€ä¸å½“ç½²å': 6,
        'å…¶ä»–è½»å¾®ä¸ç«¯è¡Œä¸º': 1
    }

    papers_df, projects_df = load_data()
    G_authors = build_networks(papers_df, projects_df, misconduct_weights)
    embeddings = deepwalk(G_authors)

    # æ„å»ºåˆ†ç±»æ•°æ®é›†
    X, y = [], []
    for edge in G_authors.edges():
        X.append(np.concatenate([embeddings[edge[0]], embeddings[edge[1]]]))
        y.append(1)

    non_edges = random.sample(list(nx.non_edges(G_authors)), len(y))
    for edge in non_edges:
        X.append(np.concatenate([embeddings[edge[0]], embeddings[edge[1]]]))
        y.append(0)

    # è®­ç»ƒåˆ†ç±»å™¨
    X = np.array(X)
    y = np.array(y)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)

    # è®¡ç®—èŠ‚ç‚¹é£é™©å€¼
    risk_scores = {node: np.linalg.norm(emb) for node, emb in embeddings.items()}

    return pd.DataFrame({
        'ä½œè€…': list(risk_scores.keys()),
        'é£é™©å€¼': list(risk_scores.values())
    }), papers_df, projects_df

# ==========================
# å¯è§†åŒ–ç•Œé¢æ¨¡å—
# ==========================
def main():
    st.set_page_config(
        page_title="ç§‘ç ”è¯šä¿¡åˆ†æå¹³å°",
        page_icon="ğŸ”¬",
        layout="wide"
    )
    
    # è‡ªå®šä¹‰CSSæ ·å¼
    st.markdown("""
    <style>
    .high-risk { color: red; font-weight: bold; animation: blink 1s infinite; }
    @keyframes blink { 0% {opacity:1;} 50% {opacity:0;} 100% {opacity:1;} }
    .metric-box { padding: 20px; border-radius: 10px; background: #f0f2f6; margin: 10px; }
    </style>
    """, unsafe_allow_html=True)

    # ä¾§è¾¹æ æ§åˆ¶é¢æ¿
    with st.sidebar:
        st.title("æ§åˆ¶é¢æ¿")
        if st.button("ğŸ”„ é‡æ–°è®¡ç®—é£é™©å€¼", help="å½“åŸå§‹æ•°æ®æ›´æ–°åç‚¹å‡»æ­¤æŒ‰é’®"):
            with st.spinner("é‡æ–°è®¡ç®—ä¸­..."):
                risk_df, papers, projects = process_risk_data()
                risk_df.to_excel('risk_scores.xlsx', index=False)
            st.success("é£é™©å€¼æ›´æ–°å®Œæˆï¼")
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½é£é™©æ•°æ®",
            data=open('risk_scores.xlsx', 'rb').read() if 'risk_df' in globals() else b'',
            file_name='ç§‘ç ”é£é™©æ•°æ®.xlsx',
            mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    # å°è¯•åŠ è½½ç°æœ‰æ•°æ®
    try:
        risk_df = pd.read_excel('risk_scores.xlsx')
        papers, projects = load_data()
    except:
        with st.spinner("é¦–æ¬¡è¿è¡Œéœ€è¦åˆå§‹åŒ–æ•°æ®..."):
            risk_df, papers, projects = process_risk_data()
            risk_df.to_excel('risk_scores.xlsx', index=False)

    # ä¸»ç•Œé¢
    st.title("ğŸ” ç§‘ç ”äººå‘˜ä¿¡ç”¨é£é™©åˆ†æç³»ç»Ÿ")
    
    # æœç´¢æ¡†
    search_term = st.text_input("è¾“å…¥ç ”ç©¶äººå‘˜å§“åï¼š", placeholder="æ”¯æŒæ¨¡ç³Šæœç´¢...")
    
    if search_term:
        # æ¨¡ç³ŠåŒ¹é…
        candidates = risk_df[risk_df['ä½œè€…'].str.contains(search_term)]
        if len(candidates) == 0:
            st.warning("æœªæ‰¾åˆ°åŒ¹é…çš„ç ”ç©¶äººå‘˜")
            return
        
        # é€‰æ‹©å…·ä½“äººå‘˜
        selected = st.selectbox("è¯·é€‰æ‹©å…·ä½“äººå‘˜ï¼š", candidates['ä½œè€…'])
        
        # è·å–è¯¦ç»†ä¿¡æ¯
        author_risk = risk_df[risk_df['ä½œè€…'] == selected].iloc[0]['é£é™©å€¼']
        paper_records = papers[papers['å§“å'] == selected]
        project_records = projects[projects['å§“å'] == selected]
        
        # ======================
        # ä¿¡æ¯å±•ç¤º
        # ======================
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“„ è®ºæ–‡è®°å½•")
            if not paper_records.empty:
                st.dataframe(paper_records, use_container_width=True)
            else:
                st.info("æš‚æ— è®ºæ–‡ä¸ç«¯è®°å½•")

        with col2:
            st.subheader("ğŸ“‹ é¡¹ç›®è®°å½•")
            if not project_records.empty:
                st.dataframe(project_records, use_container_width=True)
            else:
                st.info("æš‚æ— é¡¹ç›®ä¸ç«¯è®°å½•")

        # é£é™©æŒ‡æ ‡
        st.subheader("ğŸ“Š é£é™©åˆ†æ")
        risk_level = "high" if author_risk > 2.5 else "low"
        cols = st.columns(4)
        cols[0].metric("ä¿¡ç”¨è¯„åˆ†", f"{author_risk:.2f}", 
                      delta_color="inverse" if risk_level == "high" else "normal")
        cols[1].metric("é£é™©ç­‰çº§", 
                      f"{'âš ï¸ é«˜é£é™©' if risk_level == 'high' else 'âœ… ä½é£é™©'}",
                      help="é«˜é£é™©é˜ˆå€¼ï¼š2.5")
        
        # ======================
        # å…³ç³»ç½‘ç»œå¯è§†åŒ–
        # ======================
        with st.expander("ğŸ•¸ï¸ å±•å¼€åˆä½œå…³ç³»ç½‘ç»œ", expanded=True):
            def build_network_graph(author):
                G = nx.Graph()
                G.add_node(author, size=20, color='red')
                
                # æŸ¥æ‰¾å…³è”èŠ‚ç‚¹
                related = papers[
                    (papers['ç ”ç©¶æœºæ„'] == papers[papers['å§“å']==author]['ç ”ç©¶æœºæ„'].iloc[0]) |
                    (papers['ç ”ç©¶æ–¹å‘'] == papers[papers['å§“å']==author]['ç ”ç©¶æ–¹å‘'].iloc[0])
                ]['å§“å'].unique()
                
                for person in related:
                    if person != author:
                        G.add_node(person, size=15, color='blue')
                        G.add_edge(author, person, 
                                  title=f"å…±åŒç ”ç©¶æ–¹å‘: {papers[papers['å§“å']==person]['ç ”ç©¶æ–¹å‘'].iloc[0]}")
                
                # Plotlyå¯è§†åŒ–
                pos = nx.spring_layout(G)
                edge_x, edge_y = [], []
                for edge in G.edges():
                    x0, y0 = pos[edge[0]]
                    x1, y1 = pos[edge[1]]
                    edge_x.extend([x0, x1, None])
                    edge_y.extend([y0, y1, None])
                
                node_x = [pos[n][0] for n in G.nodes()]
                node_y = [pos[n][1] for n in G.nodes()]
                
                fig = go.Figure(
                    data=[
                        go.Scatter(
                            x=edge_x, y=edge_y,
                            line=dict(width=0.5, color='#888'),
                            hoverinfo='none',
                            mode='lines'),
                        go.Scatter(
                            x=node_x, y=node_y,
                            mode='markers+text',
                            text=list(G.nodes()),
                            textposition="top center",
                            marker=dict(
                                showscale=True,
                                colorscale='YlGnBu',
                                size=[d['size'] for d in G.nodes.values()],
                                color=[d['color'] for d in G.nodes.values()],
                                line_width=2))
                    ],
                    layout=go.Layout(
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=0,l=0,r=0,t=0),
                        xaxis=dict(showgrid=False, zeroline=False),
                        yaxis=dict(showgrid=False, zeroline=False))
                )
                st.plotly_chart(fig, use_container_width=True)
            
            build_network_graph(selected)

if __name__ == "__main__":
    main()
